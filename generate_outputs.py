import argparse
import json
import os

from core_data_modules.logging import Logger
from core_data_modules.traced_data.io import TracedDataJsonIO
from core_data_modules.util import IOUtils
from id_infrastructure.firestore_uuid_table import FirestoreUuidTable
from storage.google_cloud import google_cloud_utils
from storage.google_drive import drive_client_wrapper

from src import CombineRawDatasets, TranslateRapidProKeys, \
    AutoCodeShowAndFollowupsMessages, ProductionFile, AutoCodeDemogs, ApplyManualCodes, AnalysisFile, WSCorrection

from src.lib import PipelineConfiguration

Logger.set_project_name("IMAQAL")
log = Logger(__name__)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Runs the post-fetch phase of the ReDSS pipeline",
                                     # Support \n and long lines
                                     formatter_class=argparse.RawTextHelpFormatter)

    parser.add_argument("user", help="User launching this program")
    parser.add_argument("google_cloud_credentials_file_path", metavar="google-cloud-credentials-file-path",
                        help="Path to a Google Cloud service account credentials file to use to access the "
                             "credentials bucket")
    parser.add_argument("pipeline_configuration_file_path", metavar="pipeline-configuration-file",
                        help="Path to the pipeline configuration json file"),
    parser.add_argument("pipeline_run_mode", help="whether to generate analysis files or not", choices=["all-stages", "auto-code-only"])

    parser.add_argument("raw_data_dir", metavar="raw-data-dir",
                        help="Path to a directory containing the raw data files exported by fetch_raw_data.py")
    parser.add_argument("prev_coded_dir_path", metavar="prev-coded-dir-path",
                        help="Directory containing Coda files generated by a previous run of this pipeline. "
                             "New data will be appended to these files.")
    parser.add_argument("icr_output_dir", metavar="icr-output-dir",
                        help="Directory to write CSV files to, each containing 200 messages and message ids for use "
                             "in inter-code reliability evaluation"),
    parser.add_argument("coded_dir_path", metavar="coded-dir-path",
                        help="Directory to write coded Coda files to")

    parser.add_argument("production_csv_output_path", metavar="production-csv-output-path",
                        help="Path to a CSV file to write raw message and demographic responses to, for use in "
                             "radio show production"),
    parser.add_argument("auto_coding_json_output_path", metavar="auto-coding-json-output-path",
                        help="Path to a JSON file to write the TracedData associated with auto-coding stage of the pipeline")
    parser.add_argument("csv_by_message_output_path", metavar="csv-by-message-output-path",
                        help="Analysis dataset where messages are the unit for analysis (i.e. one message per row)")
    parser.add_argument("csv_by_individual_output_path", metavar="csv-by-individual-output-path",
                        help="Analysis dataset where respondents are the unit for analysis (i.e. one respondent "
                             "per row, with all their messages joined into a single cell)")
    parser.add_argument("messages_json_output_path", metavar="messages-json-output-path",
                        help="Path to a JSONL file to write the TracedData associated with the messages analysis file")
    parser.add_argument("individuals_json_output_path", metavar="individuals-json-output-path",
                        help="Path to a JSON file to write the TracedData associated with the individuals analysis file")

    args = parser.parse_args()

    csv_by_message_drive_path = None
    csv_by_individual_drive_path = None
    production_csv_drive_path = None

    user = args.user
    google_cloud_credentials_file_path = args.google_cloud_credentials_file_path
    pipeline_configuration_file_path = args.pipeline_configuration_file_path
    pipeline_run_mode = args.pipeline_run_mode

    raw_data_dir = args.raw_data_dir
    prev_coded_dir_path = args.prev_coded_dir_path

    icr_output_dir = args.icr_output_dir
    coded_dir_path = args.coded_dir_path
    production_csv_output_path = args.production_csv_output_path
    auto_coding_json_output_path = args.auto_coding_json_output_path
    csv_by_message_output_path = args.csv_by_message_output_path
    csv_by_individual_output_path = args.csv_by_individual_output_path
    messages_json_output_path = args.messages_json_output_path
    individuals_json_output_path = args.individuals_json_output_path

    # Load the pipeline configuration file
    log.info("Loading Pipeline Configuration File...")
    with open(pipeline_configuration_file_path) as f:
        pipeline_configuration = PipelineConfiguration.from_configuration_file(f)

    log.info("Downloading Firestore Uuid Table credentials...")
    firestore_uuid_table_credentials = json.loads(google_cloud_utils.download_blob_to_string(
        google_cloud_credentials_file_path,
        pipeline_configuration.phone_number_uuid_table.firebase_credentials_file_url
    ))
    phone_number_uuid_table = FirestoreUuidTable(
        pipeline_configuration.phone_number_uuid_table.table_name,
        firestore_uuid_table_credentials,
        "avf-phone-uuid-"
    )

    if pipeline_configuration.drive_upload is not None:
        log.info(f"Downloading Google Drive service account credentials...")
        credentials_info = json.loads(google_cloud_utils.download_blob_to_string(
            google_cloud_credentials_file_path, pipeline_configuration.drive_upload.drive_credentials_file_url))
        drive_client_wrapper.init_client_from_info(credentials_info)

    # Load messages
    messages_datasets = []
    for i, activation_flow_name in enumerate(pipeline_configuration.activation_flow_names):
        raw_activation_path = f"{raw_data_dir}/{activation_flow_name}.jsonl"
        log.info(f"Loading {raw_activation_path}...")
        with open(raw_activation_path, "r") as f:
            messages = TracedDataJsonIO.import_jsonl_to_traced_data_iterable(f)
        log.info(f"Loaded {len(messages)} runs")
        messages_datasets.append(messages)

    recovery_datasets = []
    if pipeline_configuration.recovery_csv_urls is None:
        log.debug("Not loading any recovery datasets (because the pipeline configuration json does not contain the key "
                  "'RecoveryCSVURLs')")
    else:
        log.info("Loading recovery datasets:")
        for i, recovery_csv_url in enumerate(pipeline_configuration.recovery_csv_urls):
            raw_recovery_path = f"{raw_data_dir}/{recovery_csv_url.split('/')[-1].split('.')[0]}.json"
            log.info(f"Loading {raw_recovery_path}...")
            with open(raw_recovery_path, "r") as f:
                messages = TracedDataJsonIO.import_jsonl_to_traced_data_iterable(f)
            log.info(f"Loaded {len(messages)} runs")
            recovery_datasets.append(messages)

    # Load Follow up Surveys
    follow_up_survey_datasets = []
    for i, follow_up_name in enumerate(pipeline_configuration.follow_up_flow_names):
        raw_follow_up_path = f"{raw_data_dir}/{follow_up_name}.jsonl"
        log.info(f"Loading {raw_follow_up_path}...")
        with open(raw_follow_up_path, "r") as f:
            messages = TracedDataJsonIO.import_jsonl_to_traced_data_iterable(f)
        log.info(f"Loaded {len(messages)} runs")
        follow_up_survey_datasets.append(messages)

    log.info("Loading demographics")
    demog_datasets = []
    for i, demog_flow_name in enumerate(pipeline_configuration.demog_flow_names):
        raw_demog_path = f"{raw_data_dir}/{demog_flow_name}.jsonl"
        log.info(f"Loading {raw_demog_path}...")
        with open(raw_demog_path, "r") as f:
            contacts = TracedDataJsonIO.import_jsonl_to_traced_data_iterable(f)
        log.info(f"Loaded {len(contacts)} contacts")
        demog_datasets.append(contacts)

    # Add survey data to the messages
    log.info("Combining Datasets...")
    coalesced_demog_datasets = []
    for dataset in demog_datasets:
        coalesced_demog_datasets.append(CombineRawDatasets.coalesce_traced_runs_by_key(user, dataset, "avf_phone_id"))

    coalesced_follow_up_datasets = []
    for dataset in follow_up_survey_datasets:
        coalesced_follow_up_datasets.append(CombineRawDatasets.coalesce_traced_runs_by_key(user, dataset, "avf_phone_id"))

    data = CombineRawDatasets.combine_raw_datasets(user, messages_datasets + recovery_datasets, coalesced_follow_up_datasets,
                                                   coalesced_demog_datasets)

    # Infer which RQA coding plans to use from the pipeline name.
    if pipeline_configuration.pipeline_name == "q4_pipeline":
        log.info("Running Q4 pipeline")
        PipelineConfiguration.RQA_CODING_PLANS = PipelineConfiguration.Q4_RQA_CODING_PLANS
    elif pipeline_configuration.pipeline_name == "q5_pipeline":
        log.info("Running Q5 pipeline")
        PipelineConfiguration.RQA_CODING_PLANS = PipelineConfiguration.Q5_RQA_CODING_PLANS
    else:
        assert pipeline_configuration.pipeline_name == "full_pipeline", "PipelineName must be either 'a quartely pipeline name' or 'full pipeline'"
        log.info("Running full Pipeline")
        PipelineConfiguration.RQA_CODING_PLANS = PipelineConfiguration.FULL_PIPELINE_RQA_CODING_PLANS

    log.info("Translating Rapid Pro Keys...")
    data = TranslateRapidProKeys.translate_rapid_pro_keys(user, data, pipeline_configuration, prev_coded_dir_path)

    if pipeline_configuration.move_ws_messages:
        log.info("Moving WS messages...")
        data = WSCorrection.move_wrong_scheme_messages(user, data, prev_coded_dir_path)
    else:
        log.info("Not moving WS messages (because the 'MoveWSMessages' key in the pipeline configuration "
                 "json was set to 'false')")

    log.info("Auto Coding Shows and Follow ups Messages...")
    data = AutoCodeShowAndFollowupsMessages.auto_code_show_and_followups_messages(user, data, icr_output_dir,
                                                                                  coded_dir_path)
    log.info("Exporting production CSV...")
    data = ProductionFile.generate(data, production_csv_output_path)

    log.info("Auto Coding Demogs...")
    data = AutoCodeDemogs.auto_code_demogs(user, data, phone_number_uuid_table, coded_dir_path)

    if pipeline_run_mode == "all-stages":
        log.info("Running post labelling pipeline stages...")
        log.info("Applying Manual Codes from Coda...")
        data = ApplyManualCodes.apply_manual_codes(user, data, prev_coded_dir_path)

        log.info("Generating Analysis CSVs...")
        messages_data, individuals_data = AnalysisFile.generate(user, data, csv_by_message_output_path,
                                                            csv_by_individual_output_path)

        log.info("Writing messages TracedData to file...")
        IOUtils.ensure_dirs_exist_for_file(messages_json_output_path)
        with open(messages_json_output_path, "w") as f:
            TracedDataJsonIO.export_traced_data_iterable_to_jsonl(messages_data, f)

        log.info("Writing individuals TracedData to file...")
        IOUtils.ensure_dirs_exist_for_file(individuals_json_output_path)
        with open(individuals_json_output_path, "w") as f:
            TracedDataJsonIO.export_traced_data_iterable_to_jsonl(individuals_data, f)

        # Upload to Google Drive, if requested.
        # Note: This should happen as late as possible in order to reduce the risk of the remainder of the pipeline failing
        # after a Drive upload has occurred. Failures could result in inconsistent outputs or outputs with no
        # traced data log.
        if pipeline_configuration.drive_upload is not None:
            log.info("Uploading CSVs to Google Drive...")

            production_csv_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.production_upload_path)
            production_csv_drive_file_name = os.path.basename(pipeline_configuration.drive_upload.production_upload_path)
            drive_client_wrapper.update_or_create(production_csv_output_path, production_csv_drive_dir,
                                                  target_file_name=production_csv_drive_file_name,
                                                  target_folder_is_shared_with_me=True)

            messages_csv_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.messages_upload_path)
            messages_csv_drive_file_name = os.path.basename(pipeline_configuration.drive_upload.messages_upload_path)
            drive_client_wrapper.update_or_create(csv_by_message_output_path, messages_csv_drive_dir,
                                                  target_file_name=messages_csv_drive_file_name,
                                                  target_folder_is_shared_with_me=True)

            individuals_csv_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.individuals_upload_path)
            individuals_csv_drive_file_name = os.path.basename(pipeline_configuration.drive_upload.individuals_upload_path)
            drive_client_wrapper.update_or_create(csv_by_individual_output_path, individuals_csv_drive_dir,
                                                  target_file_name=individuals_csv_drive_file_name,
                                                  target_folder_is_shared_with_me=True)

            messages_traced_data_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.messages_traced_data_upload_path)
            messages_traced_data_drive_file_name = os.path.basename(
                pipeline_configuration.drive_upload.messages_traced_data_upload_path)
            drive_client_wrapper.update_or_create(messages_json_output_path, messages_traced_data_drive_dir,
                                                  target_file_name=messages_traced_data_drive_file_name,
                                                  target_folder_is_shared_with_me=True)

            individuals_traced_data_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.individuals_traced_data_upload_path)
            individuals_traced_data_drive_file_name = os.path.basename(
                pipeline_configuration.drive_upload.individuals_traced_data_upload_path)
            drive_client_wrapper.update_or_create(individuals_json_output_path, individuals_traced_data_drive_dir,
                                                  target_file_name=individuals_traced_data_drive_file_name,
                                                  target_folder_is_shared_with_me=True)
        else:
            log.info("Skipping uploading to Google Drive (because the pipeline configuration json does not contain the key "
                     "'DriveUploadPaths')")
    else:

        assert pipeline_run_mode == "auto-code-only", "generate analysis files must be either auto-code-only or all-stages"
        log.info("Writing Auto-Coding TracedData to file...")
        IOUtils.ensure_dirs_exist_for_file(auto_coding_json_output_path)
        with open(auto_coding_json_output_path, "w") as f:
            TracedDataJsonIO.export_traced_data_iterable_to_jsonl(data, f)
        if pipeline_configuration.drive_upload is not None:
            # TODO: upload auto-coding traced data file to drive ?
            log.info("Uploading production file to Google Drive...")

            production_csv_drive_dir = os.path.dirname(pipeline_configuration.drive_upload.production_upload_path)
            production_csv_drive_file_name = os.path.basename(
                pipeline_configuration.drive_upload.production_upload_path)
            drive_client_wrapper.update_or_create(production_csv_output_path, production_csv_drive_dir,
                                                  target_file_name=production_csv_drive_file_name,
                                                  target_folder_is_shared_with_me=True)

    log.info("Python script complete")
